{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd5sNWb9WkUhPiZKkNLfaY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martadorozh/reddit/blob/main/reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Web scrapper for Reddit **\n",
        "\n",
        "Introduction Task:\n",
        "\n",
        "1.Write a Python script to scrape data from Reddit with the following parameters:\n",
        "Title,\n",
        "Content,\n",
        "Number of comments.\n",
        "\n",
        "2.Save the data in a NoSQL database like MongoDB.\n",
        "\n",
        "### **Additional functionalities:**\n",
        "\n",
        "- Configure the script to run data scraping once a day, with manual trigger option available.\n",
        "- Implement basic filtering of posts based on a single criterion, such as the number of comments."
      ],
      "metadata": {
        "id": "5FHeAVGJimfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary packages first!\n",
        "using pip install"
      ],
      "metadata": {
        "id": "vqePbIs4rxkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modules import\n",
        "import praw\n",
        "from pymongo import MongoClient\n",
        "import schedule\n",
        "from datetime import time, timedelta, datetime\n",
        "import time as tm"
      ],
      "metadata": {
        "id": "88CtV9uCiuUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    #client_id=\"insert_client_id\",\n",
        "    #client_secret=\"insert_client_secret\",\n",
        "    #user_agent=\"my-app by u/insert_username\"\n",
        ")"
      ],
      "metadata": {
        "id": "ja1C-jyejiWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MongoDB client\n",
        "client = MongoClient(\"mongodb://localhost:27017/\")\n",
        "db = client['webscarpper']  # Database name\n",
        "collection = db['reddit']     # Collection name"
      ],
      "metadata": {
        "id": "OPBANOzVjk7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddits = ['bigdata']  # choose subreddit"
      ],
      "metadata": {
        "id": "fRLG90MCjxEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch content from a subreddit\n",
        "def scrape_and_store(min_comments=1):\n",
        "    for subreddit_name in subreddits:\n",
        "        subreddit = reddit.subreddit(subreddit_name)\n",
        "        for submission in subreddit.new(limit=30):\n",
        "            if submission.num_comments >= min_comments:\n",
        "               post_data = {\n",
        "                   \"Title\": submission.title,\n",
        "                   \"Content\": submission.selftext,\n",
        "                   \"Comment count\": submission.num_comments\n",
        "               }\n",
        "               collection.insert_one(post_data)\n",
        "\n",
        "schedule.every().day.at(\"19:40\").do(scrape_and_store)\n",
        "\n",
        "\n",
        "# Manual trigger option\n",
        "def manual_trigger():\n",
        "    print(\"Manual data scraping triggered.\")\n",
        "    scrape_and_store()\n",
        "    print(\"Manual data scraping completed.\")\n",
        "\n",
        "# Run manual trigger option\n",
        "while True:\n",
        "    print(\"\\nPress 'm' to manually trigger data scraping or 'q' to quit.\")\n",
        "    choice = input(\"Enter your choice: \")\n",
        "    if choice.lower() == 'm':\n",
        "        manual_trigger()\n",
        "    elif choice.lower() == 'q':\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid choice.\")\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    tm.sleep(5)  # Check every minute"
      ],
      "metadata": {
        "id": "r4QhzxDWj4aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DVFEvl4Rirny"
      }
    }
  ]
}